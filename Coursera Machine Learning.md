# Coursera: Machine Learning

## 第一周

### Introduction

#### 机器学习（Machine Learning）的定义

从经验E中学习使得能在任务T中提升完成度P的过程

#### 监督学习（Supervised Learning） vs 无监督学习（Unsupervised Learning）

监督学习为从一些有标签的数据进行学习，然后对后续的数据添加标签

而无监督学习则是给定一些没有标签的数据，让其进行打标签

#### 分类（Classification） vs 回归（Regression）

两者都属于监督学习的范畴

可以简单理解为，分类问题中的标签是离散的，回归问题中的标签是连续的

（或认为回归问题的标签需要连续，例如说假设标签为人民币，人民币事实上可以是离散的（最低单位为1分），但是在大部分问题上该标签需要当做是连续数值来看待）

### Model and Cost Function

#### 问题

假设有一个房价预测问题，需要通过房的面积预测出房的价格

#### 一些约定

这里定义m为数据的组数（也就是行数），那么每组特征都可以表示成一个向量。例如这里的房价预测问题就可以用一个二维向量组表示：
$$
(x^{(i)},y^{(i)});i=1,...,m
$$

#### 解决问题的流程

整体流程如下图所示：

![image](img/Coursera Machine Learning/1.png)

大体思路便是，将训练数据扔进学习算法中，由学习算法生成一个函数h，然后待标记的数据集x经由函数h得到预测结果y

在这里的问题中，我们假设函数h为如下形式：
$$
h_{\theta}(x)={\theta}_0+{\theta}_1x
$$
显然，这是一个线性函数的形式，故我们用于解决这个问题所用的模型就称之为**线性回归**

由于我们的问题只有一个特征，故其称之为**单变量线性回归**

#### 单变量线性回归

单变量线性回归的模型为上述形式：
$$
h_{\theta}(x)={\theta}_0+{\theta}_1x
$$
其中θ0和θ1则是需要求出来的参数

#### 代价函数

现有的问题便是，如何利用当前已知的数据来求地最理想的函数参数

这里采取的指标便是让该函数与数据集的**均方误差和**尽量小：
$$
{\min_{\theta_0,\theta_1}}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}-y^{(i)})^2
$$
于是便可以设代价函数J为如下形式（其中除2m是后续需要）：
$$
J(\theta_0,\theta_1)={\frac {1} {2m}}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
那么问题便转换为求函数J的最小值

### Parameter Learning

#### 梯度下降法

梯度下降法的大体思路是：

+ 任意选取参数初始值
+ 重复该步骤直至参数收敛
  + 沿着当前所在的点的梯度最小的方向**同步**更新所有参数

显然，梯度下降法会找到函数的某一个极小值

假设需要用梯度下降法求的函数为：
$$
J(\theta_0,\theta_1,...,\theta_n)
$$
那么每遍迭代都可以写为如下形式：
$$
{\theta_i}:={\theta_i}-\alpha{\frac {\partial}{\partial\theta_i}}J(\theta_0,\theta_1,...,\theta_n)
$$

#### 学习率

迭代过程中的alpha即为**学习率**

学习率的大小与单次梯度下降对参数的改变的大小成正比

如果学习率太小则会出现迭代次数过多的情况

而如果学习率太大则会出现单次迭代越过最低点，甚至无法收敛的情况

#### 针对单变量线性回归的梯度下降法

对于单变量线性回归而言，其代价函数是凸的，也就是说对其运用梯度下降法可以求得全局最优解

其每次迭代需要做的更新为：
$$
\begin{cases}
\theta_0:=\theta_0-
\alpha\frac{\partial}{\partial\theta_0}
({\frac {1} {2m}}\sum_{i=1}^{m}({\theta}_0+{\theta}_1x^{(i)}-y^{(i)})^2) 
\\ 
\theta_1:=\theta_1-
\alpha\frac{\partial}{\partial\theta_1}
({\frac {1} {2m}}\sum_{i=1}^{m}({\theta}_0+{\theta}_1x^{(i)}-y^{(i)})^2)
\end{cases}
$$
求偏微分后为：
$$
\begin{cases}
\theta_0:=\theta_0-
{\frac {\alpha} {m}}
\sum_{i=1}^{m}({\theta}_0+{\theta}_1x^{(i)}-y^{(i)})
\\ 
\theta_1:=\theta_1-
{\frac {\alpha} {m}}
\sum_{i=1}^{m}(({\theta}_0+{\theta}_1x^{(i)}-y^{(i)})x^{(i)})
\end{cases}
$$

### Linear Algebra Review (Optional)

+ 向量（Vector）：可以看做是一个n*1的矩阵（本来就是）

+ 在遇到需要多重for循环计算多个结果的时候，要尽量转换为矩阵乘法
  + 因为一般语言内会封装关于矩阵乘法的诸多优化，使得其比单纯for循环要快的多
+ 只有n*n的矩阵才会有逆矩阵